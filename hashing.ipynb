{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":10479620,"sourceType":"datasetVersion","datasetId":6489054}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":25473.102831,"end_time":"2023-06-22T07:19:58.848664","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-06-22T00:15:25.745833","version":"2.4.0"},"colab":{"name":"Brain MRI generation using WGAN-GP","provenance":[],"gpuType":"V28"},"accelerator":"TPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fireducks --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:47:45.076582Z","iopub.execute_input":"2025-03-11T17:47:45.076941Z","iopub.status.idle":"2025-03-11T17:47:48.702613Z","shell.execute_reply.started":"2025-03-11T17:47:45.076909Z","shell.execute_reply":"2025-03-11T17:47:48.701425Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import fireducks.pandas as pd\nimport os\n\n# Path to the extracted chunk files (from your Kaggle dataset structure)\nextracted_chunks_path = \"/kaggle/input/processed-chunks-1\"  # Adjust if the path differs\n\n# Combine all chunk files\nall_chunks = []\nfor file_name in sorted(os.listdir(extracted_chunks_path)):  # Ensure files are combined in order\n    if file_name.startswith(\"processed_chunk_\") and file_name.endswith(\".csv\"):\n        file_path = os.path.join(extracted_chunks_path, file_name)\n        print(f\"Loading {file_name}...\")\n        chunk = pd.read_csv(file_path)\n        all_chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ncombined_data = pd.concat(all_chunks, ignore_index=True)\n\n# Display combined data info\nprint(\"Combined data shape:\", combined_data.shape)","metadata":{"id":"RAqROHmQZp-d","outputId":"2a5327c4-607d-4e14-96af-363641963b3e","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:49:23.184334Z","iopub.execute_input":"2025-03-11T17:49:23.184721Z","iopub.status.idle":"2025-03-11T17:49:40.978964Z","shell.execute_reply.started":"2025-03-11T17:49:23.184691Z","shell.execute_reply":"2025-03-11T17:49:40.977705Z"}},"outputs":[{"name":"stdout","text":"Loading processed_chunk_0_50000.csv...\nLoading processed_chunk_1000000_1050000.csv...\nLoading processed_chunk_100000_150000.csv...\nLoading processed_chunk_1050000_1100000.csv...\nLoading processed_chunk_1100000_1150000.csv...\nLoading processed_chunk_1150000_1200000.csv...\nLoading processed_chunk_1200000_1250000.csv...\nLoading processed_chunk_1250000_1300000.csv...\nLoading processed_chunk_1300000_1350000.csv...\nLoading processed_chunk_1350000_1400000.csv...\nLoading processed_chunk_1400000_1450000.csv...\nLoading processed_chunk_1450000_1500000.csv...\nLoading processed_chunk_1500000_1550000.csv...\nLoading processed_chunk_150000_200000.csv...\nLoading processed_chunk_1550000_1600000.csv...\nLoading processed_chunk_1600000_1650000.csv...\nLoading processed_chunk_1650000_1700000.csv...\nLoading processed_chunk_1700000_1750000.csv...\nLoading processed_chunk_1750000_1800000.csv...\nLoading processed_chunk_1800000_1850000.csv...\nLoading processed_chunk_1850000_1900000.csv...\nLoading processed_chunk_1900000_1950000.csv...\nLoading processed_chunk_1950000_2000000.csv...\nLoading processed_chunk_2000000_2050000.csv...\nLoading processed_chunk_200000_250000.csv...\nLoading processed_chunk_2050000_2100000.csv...\nLoading processed_chunk_2100000_2150000.csv...\nLoading processed_chunk_2150000_2200000.csv...\nLoading processed_chunk_2200000_2250000.csv...\nLoading processed_chunk_2250000_2300000.csv...\nLoading processed_chunk_2300000_2350000.csv...\nLoading processed_chunk_2350000_2400000.csv...\nLoading processed_chunk_2400000_2450000.csv...\nLoading processed_chunk_2450000_2500000.csv...\nLoading processed_chunk_2500000_2550000.csv...\nLoading processed_chunk_250000_300000.csv...\nLoading processed_chunk_2550000_2600000.csv...\nLoading processed_chunk_2600000_2650000.csv...\nLoading processed_chunk_2650000_2700000.csv...\nLoading processed_chunk_2700000_2750000.csv...\nLoading processed_chunk_2750000_2800000.csv...\nLoading processed_chunk_2800000_2850000.csv...\nLoading processed_chunk_2850000_2900000.csv...\nLoading processed_chunk_2900000_2950000.csv...\nLoading processed_chunk_2950000_3000000.csv...\nLoading processed_chunk_3000000_3050000.csv...\nLoading processed_chunk_300000_350000.csv...\nLoading processed_chunk_3050000_3100000.csv...\nLoading processed_chunk_3100000_3150000.csv...\nLoading processed_chunk_3150000_3200000.csv...\nLoading processed_chunk_3200000_3250000.csv...\nLoading processed_chunk_3250000_3300000.csv...\nLoading processed_chunk_3300000_3350000.csv...\nLoading processed_chunk_3350000_3400000.csv...\nLoading processed_chunk_3400000_3450000.csv...\nLoading processed_chunk_3450000_3500000.csv...\nLoading processed_chunk_3500000_3550000.csv...\nLoading processed_chunk_350000_400000.csv...\nLoading processed_chunk_3550000_3600000.csv...\nLoading processed_chunk_400000_450000.csv...\nLoading processed_chunk_450000_500000.csv...\nLoading processed_chunk_500000_550000.csv...\nLoading processed_chunk_50000_100000.csv...\nLoading processed_chunk_550000_600000.csv...\nLoading processed_chunk_600000_650000.csv...\nLoading processed_chunk_650000_700000.csv...\nLoading processed_chunk_700000_750000.csv...\nLoading processed_chunk_750000_800000.csv...\nLoading processed_chunk_800000_850000.csv...\nLoading processed_chunk_850000_900000.csv...\nLoading processed_chunk_900000_950000.csv...\nLoading processed_chunk_950000_1000000.csv...\nCombined data shape: (3600000, 3)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Save the combined dataset as a CSV for future use\ncombined_data_path = \"/kaggle/working/combined_processed_data.csv\"\ncombined_data.to_csv(combined_data_path, index=False)\nprint(f\"Combined data saved at: {combined_data_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:49:46.689121Z","iopub.execute_input":"2025-03-11T17:49:46.689621Z","iopub.status.idle":"2025-03-11T17:49:52.363411Z","shell.execute_reply.started":"2025-03-11T17:49:46.689591Z","shell.execute_reply":"2025-03-11T17:49:52.362297Z"}},"outputs":[{"name":"stdout","text":"Combined data saved at: /kaggle/working/combined_processed_data.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load the saved combined dataset\ncombined_data = pd.read_csv(\"/kaggle/working/combined_processed_data.csv\")\n\n# Check the dataset structure\nprint(combined_data.info())\nprint(combined_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:50:06.012874Z","iopub.execute_input":"2025-03-11T17:50:06.013176Z","iopub.status.idle":"2025-03-11T17:50:16.313646Z","shell.execute_reply.started":"2025-03-11T17:50:06.013153Z","shell.execute_reply":"2025-03-11T17:50:16.312757Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3600000 entries, 0 to 3599999\nData columns (total 3 columns):\n #   Column          Dtype \n---  ------          ----- \n 0   review          object\n 1   label           int64 \n 2   cleaned_review  object\ndtypes: int64(1), object(2)\nmemory usage: 82.4+ MB\nNone\n                                              review  label  \\\n0  Stuning even for the non-gamer: This sound tra...      2   \n1  The best soundtrack ever to anything.: I'm rea...      2   \n2  Amazing!: This soundtrack is my favorite music...      2   \n3  Excellent Soundtrack: I truly like this soundt...      2   \n4  Remember, Pull Your Jaw Off The Floor After He...      2   \n\n                                      cleaned_review  \n0  stun non gamer sound track beautiful paint sen...  \n1  good soundtrack read lot review say good game ...  \n2  amazing soundtrack favorite music time hand in...  \n3  excellent soundtrack truly like soundtrack enj...  \n4  remember pull Jaw Floor hear play game know di...  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Check label distribution\nprint(combined_data['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:50:46.856021Z","iopub.execute_input":"2025-03-11T17:50:46.856368Z","iopub.status.idle":"2025-03-11T17:50:46.903616Z","shell.execute_reply.started":"2025-03-11T17:50:46.856340Z","shell.execute_reply":"2025-03-11T17:50:46.902678Z"}},"outputs":[{"name":"stdout","text":"label\n2    1800000\n1    1800000\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\nfrom scipy.sparse import vstack\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX = combined_data['cleaned_review']  # Features (cleaned reviews)\ny = combined_data['label']           # Labels (1 for neutral, 2 for positive)\n\n# Perform train-test split (90% training, 10% testing) with a reduced test size\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")\n\n# Use HashingVectorizer for incremental vectorization\nhash_vectorizer = HashingVectorizer(n_features=5000, alternate_sign=False, ngram_range=(1, 2))\n\n# Batch processing function for HashingVectorizer\ndef batch_hash_transform(vectorizer, data, batch_size=100000):\n    \"\"\"Batch process large datasets for HashingVectorizer transformation.\"\"\"\n    batches = []\n    total = len(data)\n    for i in range(0, total, batch_size):\n        print(f\"Processing batch {i // batch_size + 1} / {total // batch_size + 1}\")\n        batch = data[i: i + batch_size].values.astype('U')  # Convert to Unicode\n        batches.append(vectorizer.transform(batch))\n    return vstack(batches)\n\n# Transform training data in batches\nprint(\"Starting HashingVectorizer transformation on training data...\")\nX_train_tfidf = batch_hash_transform(hash_vectorizer, X_train, batch_size=50000)\n\n# Transform testing data in batches\nprint(\"Starting HashingVectorizer transformation on testing data...\")\nX_test_tfidf = batch_hash_transform(hash_vectorizer, X_test, batch_size=50000)\n\nprint(\"HashingVectorizer transformation complete.\")\nprint(f\"Training Hashing shape: {X_train_tfidf.shape}\")\nprint(f\"Testing Hashing shape: {X_test_tfidf.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:50:57.562916Z","iopub.execute_input":"2025-03-11T17:50:57.563220Z","iopub.status.idle":"2025-03-11T17:53:50.205426Z","shell.execute_reply.started":"2025-03-11T17:50:57.563197Z","shell.execute_reply":"2025-03-11T17:53:50.204571Z"}},"outputs":[{"name":"stdout","text":"Training samples: 3240000\nTesting samples: 360000\nStarting HashingVectorizer transformation on training data...\nProcessing batch 1 / 65\nProcessing batch 2 / 65\nProcessing batch 3 / 65\nProcessing batch 4 / 65\nProcessing batch 5 / 65\nProcessing batch 6 / 65\nProcessing batch 7 / 65\nProcessing batch 8 / 65\nProcessing batch 9 / 65\nProcessing batch 10 / 65\nProcessing batch 11 / 65\nProcessing batch 12 / 65\nProcessing batch 13 / 65\nProcessing batch 14 / 65\nProcessing batch 15 / 65\nProcessing batch 16 / 65\nProcessing batch 17 / 65\nProcessing batch 18 / 65\nProcessing batch 19 / 65\nProcessing batch 20 / 65\nProcessing batch 21 / 65\nProcessing batch 22 / 65\nProcessing batch 23 / 65\nProcessing batch 24 / 65\nProcessing batch 25 / 65\nProcessing batch 26 / 65\nProcessing batch 27 / 65\nProcessing batch 28 / 65\nProcessing batch 29 / 65\nProcessing batch 30 / 65\nProcessing batch 31 / 65\nProcessing batch 32 / 65\nProcessing batch 33 / 65\nProcessing batch 34 / 65\nProcessing batch 35 / 65\nProcessing batch 36 / 65\nProcessing batch 37 / 65\nProcessing batch 38 / 65\nProcessing batch 39 / 65\nProcessing batch 40 / 65\nProcessing batch 41 / 65\nProcessing batch 42 / 65\nProcessing batch 43 / 65\nProcessing batch 44 / 65\nProcessing batch 45 / 65\nProcessing batch 46 / 65\nProcessing batch 47 / 65\nProcessing batch 48 / 65\nProcessing batch 49 / 65\nProcessing batch 50 / 65\nProcessing batch 51 / 65\nProcessing batch 52 / 65\nProcessing batch 53 / 65\nProcessing batch 54 / 65\nProcessing batch 55 / 65\nProcessing batch 56 / 65\nProcessing batch 57 / 65\nProcessing batch 58 / 65\nProcessing batch 59 / 65\nProcessing batch 60 / 65\nProcessing batch 61 / 65\nProcessing batch 62 / 65\nProcessing batch 63 / 65\nProcessing batch 64 / 65\nProcessing batch 65 / 65\nStarting HashingVectorizer transformation on testing data...\nProcessing batch 1 / 8\nProcessing batch 2 / 8\nProcessing batch 3 / 8\nProcessing batch 4 / 8\nProcessing batch 5 / 8\nProcessing batch 6 / 8\nProcessing batch 7 / 8\nProcessing batch 8 / 8\nHashingVectorizer transformation complete.\nTraining Hashing shape: (3240000, 5000)\nTesting Hashing shape: (360000, 5000)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, accuracy_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\n# Define all models (using GPU where applicable)\nmodels = [\n    {'name': 'Naive Bayes', 'classifier': MultinomialNB(alpha=1.0)},  # CPU only\n    {'name': 'SVM', 'classifier': LinearSVC(C=1.0, max_iter=5000)},   # CPU only\n    {'name': 'XGBoost', 'classifier': xgb.XGBClassifier(\n        use_label_encoder=False, \n        eval_metric='logloss',\n        device='cuda'  # Enable GPU\n    )},\n    {'name': 'LightGBM', 'classifier': lgb.LGBMClassifier(\n        device='gpu',  # Enable GPU\n        gpu_platform_id=0,  # Default platform\n        gpu_device_id=0     # Default device\n    )},\n    {'name': 'Random Forest', 'classifier': RandomForestClassifier(n_estimators=100)},  # CPU only\n    {'name': 'Logistic Regression', 'classifier': LogisticRegression(max_iter=1000)},  # CPU only\n    {'name': 'AdaBoost', 'classifier': AdaBoostClassifier()},  # CPU only\n    {'name': 'MLP Classifier', 'classifier': MLPClassifier(max_iter=500), 'needs_dense': True}  # CPU only\n]\n\n# Train and evaluate each model\nresults = {}\nfor model_info in models:\n    name = model_info['name']\n    classifier = model_info['classifier']\n    needs_dense = model_info.get('needs_dense', False)\n    \n    print(f\"\\nTraining {name}...\")\n    # Adjust labels for training (convert [1, 2] to [0, 1])\n    y_train_adjusted = y_train - 1\n    y_test_adjusted = y_test - 1\n    \n    if needs_dense:\n        classifier.fit(X_train_tfidf.toarray(), y_train_adjusted)\n        y_pred_adjusted = classifier.predict(X_test_tfidf.toarray())\n    else:\n        classifier.fit(X_train_tfidf, y_train_adjusted)\n        y_pred_adjusted = classifier.predict(X_test_tfidf)\n    \n    # Convert predictions back to original labels [1, 2]\n    y_pred = y_pred_adjusted + 1\n    \n    print(f\"{name} Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} Accuracy: {accuracy:.4f}\")\n    results[name] = {'y_pred': y_pred, 'accuracy': accuracy}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T18:01:16.943965Z","iopub.execute_input":"2025-03-11T18:01:16.944293Z"}},"outputs":[{"name":"stdout","text":"\nTraining Naive Bayes...\nNaive Bayes Classification Report:\n              precision    recall  f1-score   support\n\n           1       0.81      0.79      0.80    180086\n           2       0.80      0.82      0.81    179914\n\n    accuracy                           0.81    360000\n   macro avg       0.81      0.81      0.81    360000\nweighted avg       0.81      0.81      0.81    360000\n\nNaive Bayes Accuracy: 0.8057\n\nTraining SVM...\nSVM Classification Report:\n              precision    recall  f1-score   support\n\n           1       0.83      0.83      0.83    180086\n           2       0.83      0.83      0.83    179914\n\n    accuracy                           0.83    360000\n   macro avg       0.83      0.83      0.83    360000\nweighted avg       0.83      0.83      0.83    360000\n\nSVM Accuracy: 0.8331\n\nTraining XGBoost...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [18:04:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"XGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           1       0.82      0.83      0.82    180086\n           2       0.82      0.81      0.82    179914\n\n    accuracy                           0.82    360000\n   macro avg       0.82      0.82      0.82    360000\nweighted avg       0.82      0.82      0.82    360000\n\nXGBoost Accuracy: 0.8199\n\nTraining LightGBM...\n[LightGBM] [Info] Number of positive: 1620086, number of negative: 1619914\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 785756\n[LightGBM] [Info] Number of data points in the train set: 3240000, number of used features: 5000\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500027 -> initscore=0.000106\n[LightGBM] [Info] Start training from score 0.000106\nLightGBM Classification Report:\n              precision    recall  f1-score   support\n\n           1       0.81      0.81      0.81    180086\n           2       0.81      0.81      0.81    179914\n\n    accuracy                           0.81    360000\n   macro avg       0.81      0.81      0.81    360000\nweighted avg       0.81      0.81      0.81    360000\n\nLightGBM Accuracy: 0.8071\n\nTraining Random Forest...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Example: Save the SVM model (adjust based on Cell-7 or Cell-12 results)\nbest_model = LinearSVC(C=1.0, max_iter=5000)\nbest_model.fit(X_train_tfidf, y_train)  # Original labels [1, 2]\njoblib.dump(best_model, \"best_model.pkl\")\nprint(\"Best model saved as 'best_model.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:38:26.846609Z","iopub.execute_input":"2025-01-15T14:38:26.84698Z","iopub.status.idle":"2025-01-15T14:38:31.133857Z","shell.execute_reply.started":"2025-01-15T14:38:26.846949Z","shell.execute_reply":"2025-01-15T14:38:31.133141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model\nloaded_model = joblib.load(\"best_model.pkl\")\n\n# Predict on test data (no adjustment needed if model was trained on [1, 2])\nnew_predictions = loaded_model.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(\"Classification Report for Best Model:\")\nprint(classification_report(y_test, new_predictions))\n\naccuracy = accuracy_score(y_test, new_predictions)\nprint(f\"Accuracy of Best Model: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:38:44.879248Z","iopub.execute_input":"2025-01-15T14:38:44.87979Z","iopub.status.idle":"2025-01-15T14:42:31.546794Z","shell.execute_reply.started":"2025-01-15T14:38:44.879764Z","shell.execute_reply":"2025-01-15T14:42:31.545677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\n\n# Load Spacy model for preprocessing (optional, not used in current pipeline)\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:46:40.81664Z","iopub.execute_input":"2025-01-15T14:46:40.81703Z","iopub.status.idle":"2025-01-15T14:46:44.146965Z","shell.execute_reply.started":"2025-01-15T14:46:40.817004Z","shell.execute_reply":"2025-01-15T14:46:44.146075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import vstack\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\n# Define all models (same as Cell-7, with GPU for XGBoost and LightGBM)\nmodels = [\n    {'name': 'Naive Bayes', 'classifier': MultinomialNB(alpha=1.0)},\n    {'name': 'SVM', 'classifier': LinearSVC(C=1.0, max_iter=5000)},\n    {'name': 'XGBoost', 'classifier': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', device='cuda')},\n    {'name': 'LightGBM', 'classifier': lgb.LGBMClassifier(device='gpu', gpu_platform_id=0, gpu_device_id=0)},\n    {'name': 'Random Forest', 'classifier': RandomForestClassifier(n_estimators=100)},\n    {'name': 'Logistic Regression', 'classifier': LogisticRegression(max_iter=1000)},\n    {'name': 'AdaBoost', 'classifier': AdaBoostClassifier()},\n    {'name': 'MLP Classifier', 'classifier': MLPClassifier(max_iter=500), 'needs_dense': True}\n]\n\n# Number of runs for statistical analysis\nn_runs = 10\naccuracies = {model['name']: [] for model in models}\n\nfor run in range(n_runs):\n    print(f\"\\nRun {run + 1}/{n_runs}\")\n    # New random split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=run)\n    \n    # Transform data\n    X_train_tfidf = batch_hash_transform(hash_vectorizer, X_train, batch_size=50000)\n    X_test_tfidf = batch_hash_transform(hash_vectorizer, X_test, batch_size=50000)\n    \n    # Adjust labels for training (convert [1, 2] to [0, 1])\n    y_train_adjusted = y_train - 1\n    y_test_adjusted = y_test - 1\n    \n    # Train and evaluate each model\n    for model_info in models:\n        name = model_info['name']\n        classifier = model_info['classifier'].__class__()  # New instance\n        if name in ['XGBoost']:\n            classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', device='cuda')\n        elif name in ['LightGBM']:\n            classifier = lgb.LGBMClassifier(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n        needs_dense = model_info.get('needs_dense', False)\n        \n        if needs_dense:\n            classifier.fit(X_train_tfidf.toarray(), y_train_adjusted)\n            y_pred_adjusted = classifier.predict(X_test_tfidf.toarray())\n        else:\n            classifier.fit(X_train_tfidf, y_train_adjusted)\n            y_pred_adjusted = classifier.predict(X_test_tfidf)\n        \n        # Convert predictions back to [1, 2]\n        y_pred = y_pred_adjusted + 1\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies[name].append(accuracy)\n        print(f\"{name} Accuracy (Run {run + 1}): {accuracy:.4f}\")\n\n# Compute mean and standard deviation\nfor name in accuracies:\n    mean_acc = np.mean(accuracies[name])\n    std_acc = np.std(accuracies[name])\n    print(f\"\\n{name}: Mean Accuracy = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:46:54.967096Z","iopub.execute_input":"2025-01-15T14:46:54.967562Z","iopub.status.idle":"2025-01-15T14:46:55.388025Z","shell.execute_reply.started":"2025-01-15T14:46:54.967523Z","shell.execute_reply":"2025-01-15T14:46:55.386956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate 95% confidence intervals\nconfidence_intervals = {}\nfor name in accuracies:\n    mean_acc = np.mean(accuracies[name])\n    std_acc = np.std(accuracies[name])\n    se = std_acc / np.sqrt(n_runs)  # Standard error\n    ci_lower = mean_acc - 1.96 * se\n    ci_upper = mean_acc + 1.96 * se\n    confidence_intervals[name] = (ci_lower, ci_upper)\n    print(f\"{name}: 95% CI = [{ci_lower:.4f}, {ci_upper:.4f}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:47:04.570061Z","iopub.execute_input":"2025-01-15T14:47:04.570399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import wilcoxon\n\n# Perform Wilcoxon test for each pair of models\nmodel_names = list(accuracies.keys())\nfor i in range(len(model_names)):\n    for j in range(i + 1, len(model_names)):\n        model1, model2 = model_names[i], model_names[j]\n        stat, p_value = wilcoxon(accuracies[model1], accuracies[model2])\n        print(f\"\\nWilcoxon Test ({model1} vs {model2}):\")\n        print(f\"Statistic = {stat}, p-value = {p_value:.4f}\")\n        if p_value < 0.05:\n            print(f\"Significant difference between {model1} and {model2}\")\n        else:\n            print(f\"No significant difference between {model1} and {model2}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.boxplot(accuracies.values(), labels=accuracies.keys())\nplt.title(\"Performance Distribution Across Models\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nmeans = [np.mean(accuracies[name]) for name in accuracies]\nci_lowers = [confidence_intervals[name][0] for name in accuracies]\nci_uppers = [confidence_intervals[name][1] for name in accuracies]\nyerr = [(m - l, u - m) for m, l, u in zip(means, ci_lowers, ci_uppers)]\n\nplt.bar(accuracies.keys(), means, yerr=np.array(yerr).T, capsize=5, color='skyblue')\nplt.title(\"Mean Accuracy with 95% Confidence Intervals\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\n# Compute p-values matrix\np_values = np.ones((len(model_names), len(model_names)))\nfor i in range(len(model_names)):\n    for j in range(i + 1, len(model_names)):\n        _, p_value = wilcoxon(accuracies[model_names[i]], accuracies[model_names[j]])\n        p_values[i, j] = p_value\n        p_values[j, i] = p_value\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(p_values, annot=True, fmt=\".4f\", cmap=\"YlGnBu\", xticklabels=model_names, yticklabels=model_names)\nplt.title(\"Pairwise Wilcoxon Test p-values\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n\n# Store per-class metrics\nmetrics = {model['name']: {'precision_1': [], 'recall_1': [], 'f1_1': [], 'precision_2': [], 'recall_2': [], 'f1_2': []} for model in models}\n\nfor run in range(n_runs):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=run)\n    X_train_tfidf = batch_hash_transform(hash_vectorizer, X_train, batch_size=50000)\n    X_test_tfidf = batch_hash_transform(hash_vectorizer, X_test, batch_size=50000)\n    y_train_adjusted = y_train - 1\n    y_test_adjusted = y_test - 1\n    \n    for model_info in models:\n        name = model_info['name']\n        classifier = model_info['classifier'].__class__()\n        if name == 'XGBoost':\n            classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', device='cuda')\n        elif name == 'LightGBM':\n            classifier = lgb.LGBMClassifier(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n        needs_dense = model_info.get('needs_dense', False)\n        \n        if needs_dense:\n            classifier.fit(X_train_tfidf.toarray(), y_train_adjusted)\n            y_pred_adjusted = classifier.predict(X_test_tfidf.toarray())\n        else:\n            classifier.fit(X_train_tfidf, y_train_adjusted)\n            y_pred_adjusted = classifier.predict(X_test_tfidf)\n        \n        y_pred = y_pred_adjusted + 1\n        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1, 2])\n        metrics[name]['precision_1'].append(precision[0])\n        metrics[name]['recall_1'].append(recall[0])\n        metrics[name]['f1_1'].append(f1[0])\n        metrics[name]['precision_2'].append(precision[1])\n        metrics[name]['recall_2'].append(recall[1])\n        metrics[name]['f1_2'].append(f1[1])\n\n# Summarize and print\nfor name in metrics:\n    print(f\"\\n{name} Per-Class Metrics (Mean ± Std):\")\n    print(f\"Class 1 - Precision: {np.mean(metrics[name]['precision_1']):.4f} ± {np.std(metrics[name]['precision_1']):.4f}\")\n    print(f\"Class 1 - Recall: {np.mean(metrics[name]['recall_1']):.4f} ± {np.std(metrics[name]['recall_1']):.4f}\")\n    print(f\"Class 1 - F1: {np.mean(metrics[name]['f1_1']):.4f} ± {np.std(metrics[name]['f1_1']):.4f}\")\n    print(f\"Class 2 - Precision: {np.mean(metrics[name]['precision_2']):.4f} ± {np.std(metrics[name]['precision_2']):.4f}\")\n    print(f\"Class 2 - Recall: {np.mean(metrics[name]['recall_2']):.4f} ± {np.std(metrics[name]['recall_2']):.4f}\")\n    print(f\"Class 2 - F1: {np.mean(metrics[name]['f1_2']):.4f} ± {np.std(metrics[name]['f1_2']):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nbar_width = 0.35\nindex = np.arange(len(models))\n\nf1_class1 = [np.mean(metrics[name]['f1_1']) for name in metrics]\nf1_class2 = [np.mean(metrics[name]['f1_2']) for name in metrics]\n\nplt.bar(index, f1_class1, bar_width, label='Class 1 (Neutral)', color='blue')\nplt.bar(index + bar_width, f1_class2, bar_width, label='Class 2 (Positive)', color='orange')\n\nplt.xlabel('Model')\nplt.ylabel('Mean F1 Score')\nplt.title('Per-Class F1 Score Across Models')\nplt.xticks(index + bar_width / 2, [model['name'] for model in models], rotation=45)\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}